import asyncio
import os
import time

import yaml
from pywebio.input import *
from pywebio.output import *
from pywebio_battery import put_video

from app.web.views.ViewsUtils import ViewsUtils

from crawlers.hybrid.hybrid_crawler import HybridCrawler

HybridCrawler = HybridCrawler()

# è¯»å–ä¸Šçº§å†ä¸Šçº§ç›®å½•çš„é…ç½®æ–‡ä»¶
config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))), 'config.yaml')
with open(config_path, 'r', encoding='utf-8') as file:
    config = yaml.safe_load(file)


# æ ¡éªŒè¾“å…¥å€¼/Validate input value
def valid_check(input_data: str):
    # æ£€ç´¢å‡ºæ‰€æœ‰é“¾æ¥å¹¶è¿”å›åˆ—è¡¨/Retrieve all links and return a list
    url_list = ViewsUtils.find_url(input_data)
    # æ€»å…±æ‰¾åˆ°çš„é“¾æ¥æ•°é‡/Total number of links found
    total_urls = len(url_list)
    if total_urls == 0:
        warn_info = ViewsUtils.t('æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆçš„é“¾æ¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥çš„å†…å®¹æ˜¯å¦æ­£ç¡®ã€‚',
                                 'No valid link detected, please check if the input content is correct.')
        return warn_info
    else:
        # æœ€å¤§æ¥å—æäº¤URLçš„æ•°é‡/Maximum number of URLs accepted
        max_urls = config['Web']['Max_Take_URLs']
        if total_urls > int(max_urls):
            warn_info = ViewsUtils.t(f'è¾“å…¥çš„é“¾æ¥å¤ªå¤šå•¦ï¼Œå½“å‰åªä¼šå¤„ç†è¾“å…¥çš„å‰{max_urls}ä¸ªé“¾æ¥ï¼',
                                     f'Too many links input, only the first {max_urls} links will be processed!')
            return warn_info


# é”™è¯¯å¤„ç†/Error handling
def error_do(reason: str, value: str) -> None:
    # è¾“å‡ºä¸€ä¸ªæ¯«æ— ç”¨å¤„çš„ä¿¡æ¯
    put_html("<hr>")
    put_error(
        ViewsUtils.t("å‘ç”Ÿäº†ä¸€ä¸ªé”™è¯¯ï¼Œç¨‹åºå°†è·³è¿‡è¿™ä¸ªè¾“å…¥å€¼ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¾“å…¥å€¼ã€‚",
                     "An error occurred, the program will skip this input value and continue to process the next input value."))
    put_html(f"<h3>âš {ViewsUtils.t('è¯¦æƒ…', 'Details')}</h3>")
    put_table([
        [
            ViewsUtils.t('åŸå› ', 'reason'),
            ViewsUtils.t('è¾“å…¥å€¼', 'input value')
        ],
        [
            reason,
            value
        ]
    ])
    put_markdown(ViewsUtils.t('> å¯èƒ½çš„åŸå› :', '> Possible reasons:'))
    put_markdown(ViewsUtils.t("- è§†é¢‘å·²è¢«åˆ é™¤æˆ–è€…é“¾æ¥ä¸æ­£ç¡®ã€‚",
                              "- The video has been deleted or the link is incorrect."))
    put_markdown(ViewsUtils.t("- æ¥å£é£æ§ï¼Œè¯·æ±‚è¿‡äºé¢‘ç¹ã€‚",
                              "- Interface risk control, request too frequent.")),
    put_markdown(ViewsUtils.t("- æ²¡æœ‰ä½¿ç”¨æœ‰æ•ˆçš„Cookieï¼Œå¦‚æœä½ éƒ¨ç½²åæ²¡æœ‰æ›¿æ¢ç›¸åº”çš„Cookieï¼Œå¯èƒ½ä¼šå¯¼è‡´è§£æå¤±è´¥ã€‚",
                              "- No valid Cookie is used. If you do not replace the corresponding Cookie after deployment, it may cause parsing failure."))
    put_markdown(ViewsUtils.t("> å¯»æ±‚å¸®åŠ©:", "> Seek help:"))
    put_markdown(ViewsUtils.t(
        "- ä½ å¯ä»¥å°è¯•å†æ¬¡è§£æï¼Œæˆ–è€…å°è¯•è‡ªè¡Œéƒ¨ç½²é¡¹ç›®ï¼Œç„¶åæ›¿æ¢`./app/crawlers/å¹³å°æ–‡ä»¶å¤¹/config.yaml`ä¸­çš„`cookie`å€¼ã€‚",
        "- You can try to parse again, or try to deploy the project by yourself, and then replace the `cookie` value in `./app/crawlers/platform folder/config.yaml`."))

    put_markdown(
        "- GitHub Issue: [Evil0ctal/Douyin_TikTok_Download_API](https://github.com/Evil0ctal/Douyin_TikTok_Download_API/issues)")
    put_html("<hr>")


def parse_video():
    placeholder = ViewsUtils.t(
        "æ‰¹é‡è§£æè¯·ç›´æ¥ç²˜è´´å¤šä¸ªå£ä»¤æˆ–é“¾æ¥ï¼Œæ— éœ€ä½¿ç”¨ç¬¦å·åˆ†å¼€ï¼Œæ”¯æŒæŠ–éŸ³å’ŒTikToké“¾æ¥æ··åˆï¼Œæš‚æ—¶ä¸æ”¯æŒä½œè€…ä¸»é¡µé“¾æ¥æ‰¹é‡è§£æã€‚",
        "Batch parsing, please paste multiple passwords or links directly, no need to use symbols to separate, support for mixing Douyin and TikTok links, temporarily not support for author home page link batch parsing.")
    input_data = textarea(
        ViewsUtils.t('è¯·å°†æŠ–éŸ³æˆ–TikTokçš„åˆ†äº«å£ä»¤æˆ–ç½‘å€ç²˜è´´äºæ­¤',
                     "Please paste the share code or URL of [Douyin|TikTok] here"),
        type=TEXT,
        validate=valid_check,
        required=True,
        placeholder=placeholder,
        position=0)
    url_lists = ViewsUtils.find_url(input_data)
    # è§£æå¼€å§‹æ—¶é—´
    start = time.time()
    # æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
    success_count = 0
    failed_count = 0
    # é“¾æ¥æ€»æ•°
    url_count = len(url_lists)
    # è§£ææˆåŠŸçš„url
    success_list = []
    # è§£æå¤±è´¥çš„url
    failed_list = []
    # è¾“å‡ºä¸€ä¸ªæç¤ºæ¡
    with use_scope('loading_text'):
        # è¾“å‡ºä¸€ä¸ªåˆ†è¡Œç¬¦
        put_row([put_html('<br>')])
        put_warning(ViewsUtils.t('Serveré…±æ­£æ”¶åˆ°ä½ è¾“å…¥çš„é“¾æ¥å•¦ï¼(â—â€¢á´—â€¢â—)\næ­£åœ¨åŠªåŠ›å¤„ç†ä¸­ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»...',
                                 'ServerChan is receiving your input link! (â—â€¢á´—â€¢â—)\nEfforts are being made, please wait a moment...'))
    # ç»“æœé¡µæ ‡é¢˜
    put_scope('result_title')
    # éå†é“¾æ¥åˆ—è¡¨
    for url in url_lists:
        # é“¾æ¥ç¼–å·
        url_index = url_lists.index(url) + 1
        # è§£æ
        try:
            data = asyncio.run(HybridCrawler.hybrid_parsing_single_video(url, minimal=True))
        except Exception as e:
            error_msg = str(e)
            with use_scope(str(url_index)):
                error_do(reason=error_msg, value=url)
            failed_count += 1
            failed_list.append(url)
            continue

        # åˆ›å»ºä¸€ä¸ªè§†é¢‘/å›¾é›†çš„å…¬æœ‰å˜é‡
        url_type = ViewsUtils.t('è§†é¢‘', 'Video') if data.get('type') == 'video' else ViewsUtils.t('å›¾ç‰‡', 'Image')
        platform = data.get('platform')
        table_list = [
            [ViewsUtils.t('ç±»å‹', 'type'), ViewsUtils.t('å†…å®¹', 'content')],
            [ViewsUtils.t('è§£æç±»å‹', 'Type'), url_type],
            [ViewsUtils.t('å¹³å°', 'Platform'), platform],
            [f'{url_type} ID', data.get('aweme_id')],
            [ViewsUtils.t(f'{url_type}æè¿°', 'Description'), data.get('desc')],
            [ViewsUtils.t('ä½œè€…æ˜µç§°', 'Author nickname'), data.get('author').get('nickname')],
            [ViewsUtils.t('ä½œè€…ID', 'Author ID'), data.get('author').get('unique_id')],
            [ViewsUtils.t('APIé“¾æ¥', 'API URL'),
             put_link(
                 ViewsUtils.t('ç‚¹å‡»æŸ¥çœ‹', 'Click to view'),
                 f"/api/hybrid/video_data?url={url}&minimal=false",
                 new_window=True)],
            [ViewsUtils.t('APIé“¾æ¥-ç²¾ç®€', 'API URL-Minimal'),
             put_link(ViewsUtils.t('ç‚¹å‡»æŸ¥çœ‹', 'Click to view'),
                      f"/api/hybrid/video_data?url={url}&minimal=true",
                      new_window=True)]

        ]
        # å¦‚æœæ˜¯è§†é¢‘/If it's video
        if url_type == ViewsUtils.t('è§†é¢‘', 'Video'):
            # æ·»åŠ è§†é¢‘ä¿¡æ¯
            wm_video_url_HQ = data.get('video_data').get('wm_video_url_HQ')
            nwm_video_url_HQ = data.get('video_data').get('nwm_video_url_HQ')
            if wm_video_url_HQ and nwm_video_url_HQ:
                table_list.insert(4, [ViewsUtils.t('è§†é¢‘é“¾æ¥-æ°´å°', 'Video URL-Watermark'),
                                      put_link(ViewsUtils.t('ç‚¹å‡»æŸ¥çœ‹', 'Click to view'),
                                               wm_video_url_HQ, new_window=True)])
                table_list.insert(5, [ViewsUtils.t('è§†é¢‘é“¾æ¥-æ— æ°´å°', 'Video URL-No Watermark'),
                                      put_link(ViewsUtils.t('ç‚¹å‡»æŸ¥çœ‹', 'Click to view'),
                                               nwm_video_url_HQ, new_window=True)])
            table_list.insert(6, [ViewsUtils.t('è§†é¢‘ä¸‹è½½-æ°´å°', 'Video Download-Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»ä¸‹è½½', 'Click to download'),
                                           f"/api/download?url={url}&prefix=true&with_watermark=true",
                                           new_window=True)])
            table_list.insert(7, [ViewsUtils.t('è§†é¢‘ä¸‹è½½-æ— æ°´å°', 'Video Download-No-Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»ä¸‹è½½', 'Click to download'),
                                           f"/api/download?url={url}&prefix=true&with_watermark=false",
                                           new_window=True)])
            # æ·»åŠ è§†é¢‘ä¿¡æ¯
            table_list.insert(0, [
                put_video(data.get('video_data').get('nwm_video_url_HQ'), poster=None, loop=True, width='50%')])
        # å¦‚æœæ˜¯å›¾ç‰‡/If it's image
        elif url_type == ViewsUtils.t('å›¾ç‰‡', 'Image'):
            # æ·»åŠ å›¾ç‰‡ä¸‹è½½é“¾æ¥
            table_list.insert(4, [ViewsUtils.t('å›¾ç‰‡æ‰“åŒ…ä¸‹è½½-æ°´å°', 'Download images ZIP-Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»ä¸‹è½½', 'Click to download'),
                                           f"/api/download?url={url}&prefix=true&with_watermark=true",
                                           new_window=True)])
            table_list.insert(5, [ViewsUtils.t('å›¾ç‰‡æ‰“åŒ…ä¸‹è½½-æ— æ°´å°', 'Download images ZIP-No-Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»ä¸‹è½½', 'Click to download'),
                                           f"/api/download?url={url}&prefix=true&with_watermark=false",
                                           new_window=True)])
            # æ·»åŠ å›¾ç‰‡ä¿¡æ¯
            no_watermark_image_list = data.get('image_data').get('no_watermark_image_list')
            for image in no_watermark_image_list:
                table_list.append(
                    [ViewsUtils.t('å›¾ç‰‡é¢„è§ˆ(å¦‚æ ¼å¼å¯æ˜¾ç¤º): ', 'Image preview (if the format can be displayed):'),
                     put_image(image, width='50%')])
                table_list.append([ViewsUtils.t('å›¾ç‰‡ç›´é“¾: ', 'Image URL:'),
                                   put_link(ViewsUtils.t('â¬†ï¸ç‚¹å‡»æ‰“å¼€å›¾ç‰‡â¬†ï¸', 'â¬†ï¸Click to open imageâ¬†ï¸'), image,
                                            new_window=True)])
        # å‘ç½‘é¡µè¾“å‡ºè¡¨æ ¼/Put table on web page
        with use_scope(str(url_index)):
            # æ˜¾ç¤ºè¿›åº¦
            put_info(
                ViewsUtils.t(f'æ­£åœ¨è§£æç¬¬{url_index}/{url_count}ä¸ªé“¾æ¥: ',
                             f'Parsing the {url_index}/{url_count}th link: '),
                put_link(url, url, new_window=True), closable=True)
            put_table(table_list)
            put_html('<hr>')
        scroll_to(str(url_index))
        success_count += 1
        success_list.append(url)
        # print(success_count: {success_count}, success_list: {success_list}')
    # å…¨éƒ¨è§£æå®Œæˆè·³å‡ºforå¾ªç¯/All parsing completed, break out of for loop
    with use_scope('result_title'):
        put_row([put_html('<br>')])
        put_markdown(ViewsUtils.t('## ğŸ“è§£æç»“æœ:', '## ğŸ“Parsing results:'))
        put_row([put_html('<br>')])
    with use_scope('result'):
        # æ¸…é™¤è¿›åº¦æ¡
        clear('loading_text')
        # æ»šåŠ¨è‡³result
        scroll_to('result')
        # forå¾ªç¯ç»“æŸï¼Œå‘ç½‘é¡µè¾“å‡ºæˆåŠŸæé†’
        put_success(ViewsUtils.t('è§£æå®Œæˆå•¦ â™ª(ï½¥Ï‰ï½¥)ï¾‰\nè¯·æŸ¥çœ‹ä»¥ä¸‹ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æœè§‰å¾—æœ‰ç”¨çš„è¯è¯·åœ¨GitHubä¸Šå¸®æˆ‘ç‚¹ä¸€ä¸ªStarå§ï¼',
                                 'Parsing completed â™ª(ï½¥Ï‰ï½¥)ï¾‰\nPlease check the following statistics, and if you think it\'s useful, please help me click a Star on GitHub!'))
        # å°†æˆåŠŸï¼Œå¤±è´¥ä»¥åŠæ€»æ•°é‡æ˜¾ç¤ºå‡ºæ¥å¹¶ä¸”æ˜¾ç¤ºä¸ºä»£ç æ–¹ä¾¿å¤åˆ¶
        put_markdown(
            f'**{ViewsUtils.t("æˆåŠŸ", "Success")}:** {success_count} **{ViewsUtils.t("å¤±è´¥", "Failed")}:** {failed_count} **{ViewsUtils.t("æ€»æ•°é‡", "Total")}:** {success_count + failed_count}')
        # æˆåŠŸåˆ—è¡¨
        if success_count != url_count:
            put_markdown(f'**{ViewsUtils.t("æˆåŠŸåˆ—è¡¨", "Success list")}:**')
            put_code('\n'.join(success_list))
        # å¤±è´¥åˆ—è¡¨
        if failed_count > 0:
            put_markdown(f'**{ViewsUtils.t("å¤±è´¥åˆ—è¡¨", "Failed list")}:**')
            put_code('\n'.join(failed_list))
        # å°†url_listsæ˜¾ç¤ºä¸ºä»£ç æ–¹ä¾¿å¤åˆ¶
        put_markdown(ViewsUtils.t('**ä»¥ä¸‹æ˜¯æ‚¨è¾“å…¥çš„æ‰€æœ‰é“¾æ¥ï¼š**', '**The following are all the links you entered:**'))
        put_code('\n'.join(url_lists))
        # è§£æç»“æŸæ—¶é—´
        end = time.time()
        # è®¡ç®—è€—æ—¶,ä¿ç•™ä¸¤ä½å°æ•°
        time_consuming = round(end - start, 2)
        # æ˜¾ç¤ºè€—æ—¶
        put_markdown(f"**{ViewsUtils.t('è€—æ—¶', 'Time consuming')}:** {time_consuming}s")
        # æ”¾ç½®ä¸€ä¸ªæŒ‰é’®ï¼Œç‚¹å‡»åè·³è½¬åˆ°é¡¶éƒ¨
        put_button(ViewsUtils.t('å›åˆ°é¡¶éƒ¨', 'Back to top'), onclick=lambda: scroll_to('1'), color='success',
                   outline=True)
        # è¿”å›ä¸»é¡µé“¾æ¥
        put_link(ViewsUtils.t('å†æ¥ä¸€æ³¢ (ã¤Â´Ï‰`)ã¤', 'Another wave (ã¤Â´Ï‰`)ã¤'), '/')
